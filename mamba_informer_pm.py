

"""
æœ€ç»ˆä¼˜åŒ–ç‰ˆMamba-Informer
é›†æˆäº†å…ˆè¿›çš„ç‰¹å¾å­¦ä¹ å’Œæ•°æ®é¢„å¤„ç†æ–¹æ³•

æ ¸å¿ƒæ¶æ„ï¼š
1. å¢å¼ºç‰¹å¾å­¦ä¹ æ¨¡å—
2. å¤šå°ºåº¦å·ç§¯ç‰¹å¾æå–
3. çœŸæ­£çš„Mamba-InformeråŒåˆ†æ”¯ç»“æ„
   - ç®€åŒ–Mambaå—ï¼ˆçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼‰
   - Informeræ³¨æ„åŠ›æœºåˆ¶
   - é—¨æ§èåˆ
4. æ™ºèƒ½ç‰¹å¾èåˆ
5. ç¨³å®šçš„è®­ç»ƒç­–ç•¥
"""

import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
import time
from contextlib import nullcontext
from torch.cuda.amp import autocast, GradScaler
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ - ç¡®ä¿ç»“æœå¯é‡å¤
def set_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# GPUä¼˜åŒ–è®¾ç½®ï¼ˆbenchmarkæ¨¡å¼å¯èƒ½å½±å“å¯é‡å¤æ€§ï¼Œä½†èƒ½æå‡æ€§èƒ½ï¼‰
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False  # ä¸ºäº†æ€§èƒ½ï¼Œä¸å¼ºåˆ¶ç¡®å®šæ€§
if torch.cuda.is_available():
    if hasattr(torch.backends.cuda.matmul, "allow_tf32"):
        torch.backends.cuda.matmul.allow_tf32 = True
    if hasattr(torch.backends.cudnn, "allow_tf32"):
        torch.backends.cudnn.allow_tf32 = True
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision('medium')


class EnhancedFeatureLearning(nn.Module):
    """
    å¢å¼ºç‰¹å¾å­¦ä¹ æ¨¡å—
    """
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # ç‰¹å¾å˜æ¢ç½‘ç»œ
        self.feature_transform = nn.Sequential(
            nn.Linear(input_dim, output_dim * 2),
            nn.LayerNorm(output_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        # ç‰¹å¾å¢å¼º
        self.feature_enhance = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
    def forward(self, x):
        """
        å¢å¼ºç‰¹å¾å­¦ä¹ å‰å‘ä¼ æ’­
        """
        batch_size, seq_len, _ = x.shape
        
        # é‡å¡‘è¿›è¡Œç‰¹å¾å˜æ¢
        x_flat = x.reshape(-1, self.input_dim)
        
        # ç‰¹å¾å˜æ¢
        features = self.feature_transform(x_flat)
        enhanced = self.feature_enhance(features)
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        output = enhanced.reshape(batch_size, seq_len, self.output_dim)
        
        return output


class MultiScaleConv(nn.Module):
    """
    å¤šå°ºåº¦å·ç§¯æ¨¡å— - å€Ÿé‰´ConvLatent
    """
    def __init__(self, d_model):
        super().__init__()
        
        # å¤šå°ºåº¦å·ç§¯åˆ†æ”¯
        self.conv3 = nn.Conv1d(d_model, d_model//3, kernel_size=3, padding='same')
        self.conv5 = nn.Conv1d(d_model, d_model//3, kernel_size=5, padding='same')
        self.conv7 = nn.Conv1d(d_model, d_model//3, kernel_size=7, padding='same')
        
        # æ‰©å¼ å·ç§¯
        self.dilated_conv = nn.Conv1d(d_model, d_model//3, kernel_size=3, dilation=2, padding='same')
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Conv1d(d_model//3 * 4, d_model, kernel_size=1)
        
        # æ¿€æ´»å’Œæ ‡å‡†åŒ–
        self.activation = nn.GELU()
        self.norm = nn.BatchNorm1d(d_model)
        
    def forward(self, x):
        """
        å¤šå°ºåº¦å·ç§¯å‰å‘ä¼ æ’­
        """
        # x: [batch, seq_len, d_model] -> [batch, d_model, seq_len]
        x_conv = x.transpose(1, 2)
        
        # å¤šå°ºåº¦å·ç§¯
        conv3_out = self.activation(self.conv3(x_conv))
        conv5_out = self.activation(self.conv5(x_conv))
        conv7_out = self.activation(self.conv7(x_conv))
        dilated_out = self.activation(self.dilated_conv(x_conv))
        
        # æ‹¼æ¥å’Œèåˆ
        concat_features = torch.cat([conv3_out, conv5_out, conv7_out, dilated_out], dim=1)
        fused = self.fusion(concat_features)
        fused = self.norm(fused)
        fused = self.activation(fused)
        
        # è½¬å›åºåˆ—æ ¼å¼
        output = fused.transpose(1, 2)
        
        return output


class SimplifiedMambaBlock(nn.Module):
    """
    ç®€åŒ–çš„Mambaå— - ä¿æŒMambaçš„æ ¸å¿ƒæ€æƒ³
    """
    def __init__(self, d_model, d_state=16):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        
        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(d_model, d_model * 2)
        
        # å·ç§¯å±‚ (ç®€åŒ–çš„çŠ¶æ€ç©ºé—´å¤„ç†)
        self.conv1d = nn.Conv1d(
            d_model, d_model, 
            kernel_size=3, padding='same',
            groups=d_model
        )
        
        # é—¨æ§æœºåˆ¶
        self.gate_proj = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Sigmoid()
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(d_model, d_model)
        
        # æ ‡å‡†åŒ–
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        """
        ç®€åŒ–Mambaå‰å‘ä¼ æ’­
        """
        residual = x
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±å’Œåˆ†å‰²
        x_proj = self.input_proj(self.norm(x))
        x_conv, x_gate = x_proj.chunk(2, dim=-1)
        
        # å·ç§¯å¤„ç† (æ¨¡æ‹ŸçŠ¶æ€ç©ºé—´)
        x_conv_t = x_conv.transpose(1, 2)  # [B, D, L]
        conv_out = self.conv1d(x_conv_t)
        conv_out = conv_out.transpose(1, 2)  # [B, L, D]
        conv_out = F.silu(conv_out)
        
        # é—¨æ§æœºåˆ¶
        gate = self.gate_proj(x_gate)
        gated_out = conv_out * gate
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(gated_out)
        
        return residual + output


class InformerAttention(nn.Module):
    """
    Informerç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ - ä¿æŒåŸæœ‰ç»“æ„
    """
    def __init__(self, d_model, n_heads, factor=5):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.factor = factor
        self.d_k = d_model // n_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        """
        Informeræ³¨æ„åŠ›å‰å‘ä¼ æ’­
        """
        residual = x
        x = self.norm(x)
        
        B, L, D = x.shape
        H = self.n_heads
        d_k = D // H
        
        # ç¡®ä¿ç»´åº¦èƒ½æ•´é™¤
        assert D % H == 0, f"d_model ({D}) must be divisible by n_heads ({H})"
        
        Q = self.q_proj(x).view(B, L, H, d_k).transpose(1, 2)
        K = self.k_proj(x).view(B, L, H, d_k).transpose(1, 2)
        V = self.v_proj(x).view(B, L, H, d_k).transpose(1, 2)
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        scores = torch.einsum('bhid,bhjd->bhij', Q, K) / (d_k ** 0.5)
        attn = F.softmax(scores, dim=-1)
        context = torch.einsum('bhij,bhjd->bhid', attn, V)
        
        context = context.transpose(1, 2).contiguous().view(B, L, D)
        output = self.out_proj(context)
        
        return residual + output



class OptimizedMambaInformerBlock(nn.Module):
    """
    ä¼˜åŒ–çš„Mamba-Informerå— - ç›´æ¥æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œé¿å…é‡å¤
    """
    def __init__(self, d_model, n_heads=8, dropout=0.1):
        super().__init__()
        
        # Mambaåˆ†æ”¯
        self.mamba = SimplifiedMambaBlock(d_model)
        
        # Informeræ³¨æ„åŠ›åˆ†æ”¯
        self.informer_attn = InformerAttention(d_model, n_heads)
        
        # é—¨æ§èåˆ
        self.gate = nn.Parameter(torch.tensor(0.5))
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        """
        ä¼˜åŒ–å—å‰å‘ä¼ æ’­ - ç®€åŒ–ç‰ˆæœ¬
        """
        # Mambaåˆ†æ”¯
        mamba_out = self.mamba(x)
        
        # Informeråˆ†æ”¯
        informer_out = self.informer_attn(x)
        
        # é—¨æ§èåˆ
        fused = self.gate * mamba_out + (1 - self.gate) * informer_out
        
        # å‰é¦ˆç½‘ç»œ
        output = fused + self.ffn(fused)
        
        return output


class FinalOptimizedModel(nn.Module):
    """
    æœ€ç»ˆä¼˜åŒ–çš„Mamba-Informeræ¨¡å‹
    ä¸“æ³¨äºå®é™…æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥
    """
    def __init__(self, input_dim, d_model=128, n_layers=4, seq_len=24, pred_len=1, dropout=0.15):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.d_model = d_model
        
        # 1. å¢å¼ºç‰¹å¾å­¦ä¹ 
        self.feature_learning = EnhancedFeatureLearning(input_dim, d_model)
        
        # 2. ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(seq_len, d_model) * 0.01)
        
        # 3. ä¼˜åŒ–çš„Mamba-Informerå±‚
        self.layers = nn.ModuleList([
            OptimizedMambaInformerBlock(d_model, n_heads=8, dropout=dropout)
            for _ in range(n_layers)
        ])
        
        # 4. ç‰¹å¾èšåˆ
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        
        # 5. é¢„æµ‹å¤´
        self.prediction_head = nn.Sequential(
            nn.Linear(d_model * 3, d_model),  # åºåˆ—+å¹³å‡+æœ€å¤§ç‰¹å¾
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, pred_len)
        )
        
        # 6. æ®‹å·®é¢„æµ‹
        self.linear_residual = nn.Linear(input_dim, pred_len)
        self.ar_residual = nn.Linear(1, pred_len)
        
        # 7. èåˆæƒé‡
        self.fusion_weights = nn.Parameter(torch.tensor([0.8, 0.15, 0.05]))
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
    
    def forward(self, x):
        """
        æœ€ç»ˆä¼˜åŒ–çš„å‰å‘ä¼ æ’­
        """
        batch_size, seq_len, _ = x.shape
        x_raw = x
        
        # 1. å¢å¼ºç‰¹å¾å­¦ä¹ 
        features = self.feature_learning(x)
        
        # 2. ä½ç½®ç¼–ç 
        features = features + self.pos_embedding[:seq_len].unsqueeze(0)
        
        # 3. é€šè¿‡ä¼˜åŒ–å±‚
        for layer in self.layers:
            features = layer(features)
        
        # 4. å¤šå±‚æ¬¡ç‰¹å¾èšåˆ
        # åºåˆ—ç‰¹å¾ (æœ€åæ—¶åˆ»)
        seq_features = features[:, -1, :]  # [batch, d_model]
        
        # å…¨å±€ç‰¹å¾
        features_conv = features.transpose(1, 2)  # [batch, d_model, seq_len]
        global_avg = self.global_pool(features_conv).squeeze(-1)  # [batch, d_model]
        global_max = self.max_pool(features_conv).squeeze(-1)  # [batch, d_model]
        
        # èåˆç‰¹å¾
        combined_features = torch.cat([seq_features, global_avg, global_max], dim=1)
        
        # 5. ä¸»é¢„æµ‹
        main_pred = self.prediction_head(combined_features)
        
        # 6. æ®‹å·®é¢„æµ‹
        linear_pred = self.linear_residual(x_raw[:, -1, :])
        ar_pred = self.ar_residual(x_raw[:, -1, -1].unsqueeze(-1))
        
        # 7. æ™ºèƒ½èåˆ
        weights = F.softmax(self.fusion_weights, dim=0)
        final_pred = (weights[0] * main_pred + 
                     weights[1] * linear_pred + 
                     weights[2] * ar_pred)
        
        return final_pred


def train_final_model(model, train_loader, val_loader, epochs=60, lr=0.001, patience=12):
    """
    æœ€ç»ˆä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    use_cuda = device.type == 'cuda'
    gpu_name = None
    if use_cuda:
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"æ£€æµ‹åˆ°GPU: {gpu_name}")
        print(f"æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(current_device).total_memory / 1024**3:.2f} GB")
        
        # ç¦ç”¨torch.compileï¼Œé¿å…Tritonä¾èµ–é—®é¢˜
        # if hasattr(torch, "compile"):
        #     try:
        #         model = torch.compile(model, mode="max-autotune")
        #         print("å·²å¯ç”¨torch.compileåŠ é€Ÿ")
        #     except Exception as compile_error:
        #         print(f"torch.compileåŠ é€Ÿå¤±è´¥: {compile_error}")
    else:
        print("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œè®­ç»ƒå°†åœ¨CPUä¸Šè¿›è¡Œã€‚")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        weight_decay=5e-4,
        betas=(0.9, 0.95)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=15, T_mult=2, eta_min=lr*0.01
    )
    
    # æŸå¤±å‡½æ•°
    mse_criterion = nn.MSELoss()
    huber_criterion = nn.SmoothL1Loss(beta=0.5)
    
    scaler = GradScaler(enabled=use_cuda)
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    
    print(f"å¼€å§‹è®­ç»ƒæœ€ç»ˆä¼˜åŒ–æ¨¡å‹ï¼Œè®¾å¤‡: {device}")
    print(f"æ¨¡å‹æ˜¯å¦åœ¨CUDAä¸Š: {next(model.parameters()).is_cuda}")
    
    for epoch in range(epochs):
        if use_cuda:
            torch.cuda.synchronize()
        epoch_start_time = time.time()  # è®°å½•æœ¬è½®å¼€å§‹æ—¶é—´
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        batch_count = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device, non_blocking=True)
            batch_y = batch_y.to(device, non_blocking=True)
            
            # ç¬¬ä¸€ä¸ªbatchæ‰“å°è°ƒè¯•ä¿¡æ¯
            if epoch == 0 and batch_count == 0:
                print(f"ç¬¬ä¸€ä¸ªbatchæ•°æ®æ˜¯å¦åœ¨CUDAä¸Š: {batch_x.is_cuda}")
                if use_cuda:
                    print(f"å½“å‰GPUæ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
            
            optimizer.zero_grad()
            
            amp_context = autocast(dtype=torch.float16) if use_cuda else nullcontext()
            with amp_context:
                predictions = model(batch_x)
                
                # æ··åˆæŸå¤±
                mse_loss = mse_criterion(predictions.squeeze(), batch_y.squeeze())
                huber_loss = huber_criterion(predictions.squeeze(), batch_y.squeeze())
                loss = 0.7 * mse_loss + 0.3 * huber_loss
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            batch_count += 1
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_preds = []
        val_trues = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x = batch_x.to(device, non_blocking=True)
                batch_y = batch_y.to(device, non_blocking=True)
                
                amp_context = autocast(dtype=torch.float16) if use_cuda else nullcontext()
                with amp_context:
                    predictions = model(batch_x)
                    loss = mse_criterion(predictions.squeeze(), batch_y.squeeze())
                
                val_loss += loss.item()
                val_preds.append(predictions.detach().cpu())
                val_trues.append(batch_y.detach().cpu())
        
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        val_preds = torch.cat(val_preds).squeeze().numpy()
        val_trues = torch.cat(val_trues).squeeze().numpy()
        val_r2 = r2_score(val_trues, val_preds)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        scheduler.step()
        
        # è®¡ç®—æœ¬è½®è®­ç»ƒæ—¶é—´
        if use_cuda:
            torch.cuda.synchronize()
        epoch_time = time.time() - epoch_start_time
        
        # æ—©åœå’Œä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'model/mamba_informer_pm.pth')
        else:
            patience_counter += 1
        
        print(f'Epoch [{epoch + 1:2d}/{epochs}] '
              f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | RÂ²: {val_r2:.4f} | '
              f'LR: {optimizer.param_groups[0]["lr"]:.6f} | Time: {epoch_time:.2f}s'
              f'{" | GPU: " + gpu_name if gpu_name else ""}')
        
        if patience_counter >= patience:
            print(f"æ—©åœ: {patience}è½®æ— æ”¹å–„")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('model/mamba_informer_pm.pth'))
    return model, train_losses, val_losses


def main():
    """
    ä¸»å‡½æ•° - æœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬
    """
    # ç¡®ä¿éšæœºç§å­ç”Ÿæ•ˆ
    set_seed(42)
    
    print("=" * 60)
    print("æœ€ç»ˆä¼˜åŒ–Mamba-Informer - ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜")
    print("=" * 60)
    print("ğŸ”§ æ–°æ•°æ®æµç¨‹: åŸºç¡€é¢„å¤„ç† â†’ æ•°æ®åˆ’åˆ† â†’ åˆ†åˆ«ç‰¹å¾å·¥ç¨‹ â†’ æ ‡å‡†åŒ– â†’ è®­ç»ƒ")
    print("âœ… å·²ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜ï¼Œç¡®ä¿è®­ç»ƒé›†ä¸ä½¿ç”¨æµ‹è¯•é›†ä¿¡æ¯")
    
    # åŸºç¡€æ•°æ®é¢„å¤„ç† - ä¸åŒ…å«ç‰¹å¾å·¥ç¨‹ï¼Œé¿å…æ•°æ®æ³„éœ²
    data = preprocess_basic_data('metro.xls')

    # æ¨¡å‹å‚æ•°
    seq_len = 24
    batch_size = 32
    epochs = 60
    lr = 0.001

    # æ•°æ®åˆ’åˆ†
    train_dataset, test_dataset, scaler = create_data_splits(
        data, seq_len=seq_len, test_size=0.3, augment_factor=2
    )
    
    # è·å–å®é™…ç‰¹å¾ç»´åº¦ï¼ˆä»æ•°æ®é›†ä¸­è·å–ï¼‰
    sample_x, sample_y = train_dataset[0]
    actual_input_dim = sample_x.shape[-1]  # å®é™…ç‰¹å¾æ•°é‡
    print(f"å®é™…è¾“å…¥ç‰¹å¾ç»´åº¦: {actual_input_dim}")
    
    # éªŒè¯é›†åˆ’åˆ†
    train_size = int(len(train_dataset) * 0.8)
    train_indices = list(range(train_size))
    val_indices = list(range(train_size, len(train_dataset)))
    
    train_subset = torch.utils.data.Subset(train_dataset, train_indices)
    val_subset = torch.utils.data.Subset(train_dataset, val_indices)
    
    # DataLoader
    # æ ¹æ®ç¯å¢ƒåŠ¨æ€è®¾ç½® DataLoader ä¼˜åŒ–å‚æ•°
    # Windows å¹³å°è®¾ç½® num_workers=0 é¿å…å¤šè¿›ç¨‹å†…å­˜é—®é¢˜
    import platform
    is_windows = platform.system() == 'Windows'
    
    if is_windows:
        # Windows å¹³å°ï¼šé¿å…å¤šè¿›ç¨‹å†…å­˜é”™è¯¯ (WinError 1455)
        num_workers = 0
        pin_memory = torch.cuda.is_available()
        persistent_workers = False
        prefetch_factor = None
    elif torch.cuda.is_available():
        # é Windows ä¸”æœ‰ GPU
        num_workers = min(8, os.cpu_count() or 4)  # æœ€å¤š8ä¸ªworkersï¼Œä¸æŠ¥å‘Šä¸€è‡´
        pin_memory = True
        persistent_workers = True
        prefetch_factor = 4
    else:
        # é Windows ä¸”æ—  GPU
        num_workers = max(os.cpu_count() - 1, 1)
        pin_memory = False
        persistent_workers = False
        prefetch_factor = 2

    # åˆ›å»ºéšæœºæ•°ç”Ÿæˆå™¨ä»¥ç¡®ä¿å¯é‡å¤æ€§
    g = torch.Generator()
    g.manual_seed(42)
    
    # åˆ›å»º DataLoaderï¼Œé’ˆå¯¹ Windows å¹³å°è¿›è¡Œå‚æ•°é€‚é…
    train_loader_kwargs = {
        'batch_size': batch_size,
        'shuffle': True,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'drop_last': True,
        'generator': g  # ç¡®ä¿ shuffle çš„å¯é‡å¤æ€§
    }
    if not is_windows:
        train_loader_kwargs['persistent_workers'] = persistent_workers
        train_loader_kwargs['prefetch_factor'] = prefetch_factor
    
    val_test_loader_kwargs = {
        'batch_size': batch_size,
        'shuffle': False,
        'num_workers': num_workers,
        'pin_memory': pin_memory
    }
    if not is_windows:
        val_test_loader_kwargs['persistent_workers'] = persistent_workers
        val_test_loader_kwargs['prefetch_factor'] = prefetch_factor

    train_loader = DataLoader(train_subset, **train_loader_kwargs)
    val_loader = DataLoader(val_subset, **val_test_loader_kwargs)
    test_loader = DataLoader(test_dataset, **val_test_loader_kwargs)
    
    # åˆ›å»ºæœ€ç»ˆä¼˜åŒ–æ¨¡å‹ - ä½¿ç”¨å®é™…ç‰¹å¾ç»´åº¦
    input_dim = actual_input_dim
    model = FinalOptimizedModel(
        input_dim=input_dim,
        d_model=128,
        n_layers=4,
        seq_len=seq_len,
        pred_len=1,
        dropout=0.15
    )
    
    print(f"æœ€ç»ˆæ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # è®­ç»ƒæ¨¡å‹
    model, train_losses, val_losses = train_final_model(
        model, train_loader, val_loader, epochs=epochs, lr=lr, patience=12
    )
    
    # è¯„ä¼°æ¨¡å‹
    r2, mse, mae, rmse, predictions, true_values = evaluate_model(
        model, test_loader, scaler
    )
    
    # print(f"\nğŸ¯ æœ€ç»ˆä¼˜åŒ–Mamba-Informerç»“æœ:")
    # print(f"æµ‹è¯•é›† RÂ² Score: {r2:.4f}")
    # print(f"MSE: {mse:.4f}")
    # print(f"MAE: {mae:.4f}")
    # print(f"RMSE: {rmse:.4f}")
    
    # # æ€§èƒ½åˆ†æ
    # print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    # print(f"ä¼˜åŒ–ç‰ˆæœ¬:      {r2:.4f}")

    # if r2 >= 0.95:
    #     print(f"ğŸ‰ ä¼˜ç§€ï¼è¾¾åˆ°é«˜æ€§èƒ½æ ‡å‡†ï¼")
    # elif r2 >= 0.90:
    #     print(f"âœ… è‰¯å¥½ï¼æ˜¾è‘—æ”¹å–„ï¼")
    # else:
    #     print(f"ğŸ“ˆ æœ‰æ”¹å–„ï¼Œç»§ç»­ä¼˜åŒ–ä¸­")
    
    # ä¿å­˜ç»“æœ
    results = {
        'r2': r2,
        'mse': mse,
        'mae': mae,
        'rmse': rmse,
        'predictions': predictions,
        'true_values': true_values
    }
    
    np.save('result/mamba_informer_pm.npy', results)
    
    return model, results


class TimeSeriesDataset(Dataset):
    """
    æ—¶é—´åºåˆ—æ•°æ®é›†ç±»
    """
    def __init__(self, data, seq_len=24, pred_len=1, augment=False):
        self.data = data
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.augment = augment

        # åˆ›å»ºåºåˆ—
        self.sequences = []
        self.targets = []

        for i in range(len(data) - seq_len - pred_len + 1):
            seq = data[i:i+seq_len]
            target = data[i+seq_len:i+seq_len+pred_len, -1]  # PM2.5æ˜¯æœ€åä¸€åˆ—
            self.sequences.append(seq)
            self.targets.append(target)

        self.sequences = np.array(self.sequences)
        self.targets = np.array(self.targets)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq = self.sequences[idx]
        target = self.targets[idx]

        # æ•°æ®å¢å¼º
        if self.augment and np.random.random() < 0.4:
            noise = np.random.normal(0, 0.01, seq.shape)
            seq = seq + noise

        return torch.FloatTensor(seq), torch.FloatTensor(target)


def preprocess_basic_data(file_path):
    """
    åŸºç¡€æ•°æ®é¢„å¤„ç† - ä¸åŒ…å«ç‰¹å¾å·¥ç¨‹ï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    print("å¼€å§‹åŸºç¡€æ•°æ®é¢„å¤„ç†...")

    # è¯»å–æ•°æ®
    data = pd.read_excel(file_path)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")

    # ç§»é™¤ç¬¬ä¸€è¡Œï¼ˆå•ä½è¡Œï¼‰å’Œæœ€åä¸€åˆ—ï¼ˆå¤‡æ³¨åˆ—ï¼‰
    data = data.iloc[1:, :-1].reset_index(drop=True)

    # é‡å‘½ååˆ—
    columns = ['æ—¶é—´', 'NO', 'NO2', 'CO', 'CO2', 'TEMP', 'HUM', 'PM10', 'PM2.5']
    data.columns = columns

    # ç§»é™¤æ—¶é—´åˆ—ç”¨äºå»ºæ¨¡
    feature_data = data.iloc[:, 1:].copy()

    # è½¬æ¢æ•°æ®ç±»å‹
    for col in feature_data.columns:
        feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')

    # å¤„ç†ç¼ºå¤±å€¼ - åªä½¿ç”¨å‰å‘å¡«å……ï¼Œé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
    print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{feature_data.isnull().sum()}")
    feature_data = feature_data.fillna(method='ffill')  # åªç”¨å‰å‘å¡«å……
    feature_data = feature_data.dropna()

    print(f"åŸºç¡€é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {feature_data.shape}")
    print(f"PM2.5ç»Ÿè®¡ä¿¡æ¯:\n{feature_data['PM2.5'].describe()}")

    return feature_data


def apply_feature_engineering(data, start_idx=0):
    """
    åº”ç”¨ç‰¹å¾å·¥ç¨‹ - ç¡®ä¿ä¸ä½¿ç”¨æœªæ¥ä¿¡æ¯
    """
    print(f"å¯¹ç´¢å¼• {start_idx} å¼€å§‹çš„æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    
    feature_data = data.copy()
    
    # æ·»åŠ æ ¸å¿ƒæ»åç‰¹å¾
    for lag in [1, 2, 3, 6, 12, 24]:
        feature_data[f'PM2.5_lag_{lag}'] = feature_data['PM2.5'].shift(lag)
        if lag <= 6:
            feature_data[f'PM10_lag_{lag}'] = feature_data['PM10'].shift(lag)
            feature_data[f'NO_lag_{lag}'] = feature_data['NO'].shift(lag)
            feature_data[f'NO2_lag_{lag}'] = feature_data['NO2'].shift(lag)

    # æ·»åŠ ç§»åŠ¨å¹³å‡ç‰¹å¾ - åªä½¿ç”¨å†å²çª—å£
    for window in [3, 6, 12, 24]:
        feature_data[f'PM2.5_ma_{window}'] = feature_data['PM2.5'].rolling(window=window, min_periods=1).mean()
        feature_data[f'TEMP_ma_{window}'] = feature_data['TEMP'].rolling(window=window, min_periods=1).mean()
        feature_data[f'HUM_ma_{window}'] = feature_data['HUM'].rolling(window=window, min_periods=1).mean()

    # æ·»åŠ å·®åˆ†ç‰¹å¾
    feature_data['PM2.5_diff_1'] = feature_data['PM2.5'].diff(1)
    feature_data['PM2.5_diff_3'] = feature_data['PM2.5'].diff(3)
    feature_data['PM10_diff'] = feature_data['PM10'].diff()

    # æ·»åŠ æ¯”ç‡ç‰¹å¾
    feature_data['PM_ratio'] = feature_data['PM2.5'] / (feature_data['PM10'] + 1e-8)
    feature_data['NO_NO2_ratio'] = feature_data['NO'] / (feature_data['NO2'] + 1e-8)

    # æ·»åŠ äº¤äº’ç‰¹å¾
    feature_data['temp_hum_interaction'] = feature_data['TEMP'] * feature_data['HUM']
    feature_data['pollution_index'] = (feature_data['PM2.5'] + feature_data['PM10'] +
                                     feature_data['NO'] + feature_data['NO2']) / 4

    # æ·»åŠ æ—¶é—´ç‰¹å¾ - åŸºäºç›¸å¯¹ç´¢å¼•ä½ç½®
    relative_idx = np.arange(len(feature_data)) + start_idx
    feature_data['hour_sin'] = np.sin(2 * np.pi * (relative_idx % 24) / 24)
    feature_data['hour_cos'] = np.cos(2 * np.pi * (relative_idx % 24) / 24)

    # ç§»é™¤NaNè¡Œ
    feature_data = feature_data.dropna()
    
    print(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®å½¢çŠ¶: {feature_data.shape}")
    return feature_data


def create_data_splits(data, seq_len=24, test_size=0.3, augment_factor=1):
    """
    åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ† - å…ˆåˆ’åˆ†å†ç‰¹å¾å·¥ç¨‹ï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    print(f"åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®...")

    # è®¡ç®—åˆ’åˆ†ç‚¹
    total_len = len(data)
    max_seq_start = total_len - seq_len
    train_size = int(max_seq_start * (1 - test_size))

    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†åŸå§‹æ•°æ®
    train_data_raw = data.iloc[:train_size + seq_len].copy()
    test_data_raw = data.iloc[train_size:].copy()
    
    print(f"åŸå§‹è®­ç»ƒæ•°æ®å½¢çŠ¶: {train_data_raw.shape}")
    print(f"åŸå§‹æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data_raw.shape}")

    # åˆ†åˆ«å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œç‰¹å¾å·¥ç¨‹ - é¿å…æ•°æ®æ³„éœ²
    print("\nå¯¹è®­ç»ƒé›†è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    train_data_engineered = apply_feature_engineering(train_data_raw, start_idx=0)
    
    print("\nå¯¹æµ‹è¯•é›†è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    test_data_engineered = apply_feature_engineering(test_data_raw, start_idx=train_size)
    
    # ç¡®ä¿ç‰¹å¾æ•°é‡ä¸€è‡´
    common_features = list(set(train_data_engineered.columns) & set(test_data_engineered.columns))
    train_data_engineered = train_data_engineered[common_features]
    test_data_engineered = test_data_engineered[common_features]
    
    print(f"æœ€ç»ˆç‰¹å¾æ•°é‡: {len(common_features)}")

    # æ ‡å‡†åŒ– - åªåœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆ
    print("\nè¿›è¡Œæ ‡å‡†åŒ–...")
    scaler = RobustScaler(quantile_range=(5.0, 95.0))
    train_scaled = scaler.fit_transform(train_data_engineered.values)
    test_scaled = scaler.transform(test_data_engineered.values)

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TimeSeriesDataset(train_scaled, seq_len=seq_len, augment=(augment_factor > 1))
    test_dataset = TimeSeriesDataset(test_scaled, seq_len=seq_len, augment=False)

    # æ•°æ®å¢å¼º
    if augment_factor > 1:
        augmented_datasets = [train_dataset]
        for _ in range(augment_factor - 1):
            aug_dataset = TimeSeriesDataset(train_scaled, seq_len=seq_len, augment=True)
            augmented_datasets.append(aug_dataset)

        train_dataset = torch.utils.data.ConcatDataset(augmented_datasets)

    print(f"æœ€ç»ˆè®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"æœ€ç»ˆæµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

    return train_dataset, test_dataset, scaler


def evaluate_model(model, test_loader, scaler):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()

    predictions = []
    true_values = []

    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x = batch_x.to(device, non_blocking=True)
            batch_y = batch_y.to(device, non_blocking=True)

            output = model(batch_x)
            predictions.extend(output.cpu().numpy().flatten())
            true_values.extend(batch_y.cpu().numpy().flatten())

    predictions = np.array(predictions)
    true_values = np.array(true_values)

    # åæ ‡å‡†åŒ–
    dummy_pred = np.zeros((len(predictions), scaler.n_features_in_))
    dummy_true = np.zeros((len(true_values), scaler.n_features_in_))

    dummy_pred[:, -1] = predictions
    dummy_true[:, -1] = true_values

    predictions_rescaled = scaler.inverse_transform(dummy_pred)[:, -1]
    true_values_rescaled = scaler.inverse_transform(dummy_true)[:, -1]

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    r2 = r2_score(true_values_rescaled, predictions_rescaled)
    mse = mean_squared_error(true_values_rescaled, predictions_rescaled)
    mae = mean_absolute_error(true_values_rescaled, predictions_rescaled)
    rmse = np.sqrt(mse)

    print(f"\næ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"RÂ² Score: {r2:.4f}")
    print(f"MSE: {mse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")

    return r2, mse, mae, rmse, predictions_rescaled, true_values_rescaled


if __name__ == "__main__":
    import os
    # Windows å¹³å°å¤šè¿›ç¨‹æ”¯æŒ
    import multiprocessing
    multiprocessing.freeze_support()
    
    os.makedirs('model', exist_ok=True)
    os.makedirs('result', exist_ok=True)

    main()
