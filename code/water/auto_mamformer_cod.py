"""
Auto-Mamformeræ¨¡å‹ - åºŸæ°´å¤„ç†CODé¢„æµ‹ç‰ˆæœ¬
Mamba + Autoformeræ··åˆæ¶æ„

æ ¸å¿ƒæ¶æ„ï¼š
1. å¢å¼ºç‰¹å¾å­¦ä¹ æ¨¡å—
2. å¤šå°ºåº¦å·ç§¯ç‰¹å¾æå–
3. Auto-MamformeråŒåˆ†æ”¯ç»“æ„
   - Mambaå—ï¼ˆçŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼‰
   - Autoformeræœºåˆ¶ï¼ˆè‡ªç›¸å…³ + åºåˆ—åˆ†è§£ï¼‰
   - é—¨æ§èåˆ
4. æ™ºèƒ½ç‰¹å¾èåˆ
5. ç¨³å®šçš„è®­ç»ƒç­–ç•¥

æ•°æ®è¯´æ˜ï¼š
- è¾“å…¥ï¼šåºŸæ°´å¤„ç†å‚å„é¡¹æŒ‡æ ‡ï¼ˆæµé‡ã€PHã€BODã€CODã€SSç­‰ï¼‰
- ç›®æ ‡ï¼šé¢„æµ‹COD-Sï¼ˆäºŒæ²‰æ± å‡ºæ°´CODï¼‰
- ç‰¹å¾é€‰æ‹©ï¼šåŸºäºç›¸å…³æ€§å’Œä¸“ä¸šçŸ¥è¯†é€‰æ‹©å…³é”®è¾“å…¥å˜é‡
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import warnings
import time
from contextlib import nullcontext
from torch.cuda.amp import autocast, GradScaler
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
import matplotlib as mpl
import platform

# æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©åˆé€‚çš„ä¸­æ–‡å­—ä½“
system = platform.system()
if system == 'Windows':
    # Windowsç³»ç»Ÿ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi']
elif system == 'Darwin':
    # macOSç³»ç»Ÿ
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang SC', 'STHeiti']
else:
    # Linuxç³»ç»Ÿ
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'Droid Sans Fallback']

# è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['axes.unicode_minus'] = False


def resolve_water_data_path(filename='water-treatment_model_cleaned.csv'):
    """è§£æwateræ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå…¼å®¹ä»ä¸åŒå·¥ä½œç›®å½•å¯åŠ¨è„šæœ¬ã€‚"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    repo_root = os.path.abspath(os.path.join(script_dir, '..', '..'))

    candidates = [
        filename,
        os.path.join(script_dir, filename),
        os.path.join(repo_root, filename),
        os.path.join(repo_root, 'data', 'water', filename),
    ]

    for path in candidates:
        if os.path.exists(path):
            return path

    raise FileNotFoundError(
        f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {filename}\n"
        f"å·²æ£€æŸ¥è·¯å¾„:\n- " + "\n- ".join(candidates)
    )

# è®¾ç½®éšæœºç§å­ - ç¡®ä¿ç»“æœå¯é‡å¤
def set_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_seed(42)

# GPUä¼˜åŒ–è®¾ç½®
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
if torch.cuda.is_available():
    if hasattr(torch.backends.cuda.matmul, "allow_tf32"):
        torch.backends.cuda.matmul.allow_tf32 = True
    if hasattr(torch.backends.cudnn, "allow_tf32"):
        torch.backends.cudnn.allow_tf32 = True
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision('medium')


class SeriesDecomp(nn.Module):
    """
    åºåˆ—åˆ†è§£æ¨¡å— - Autoformeræ ¸å¿ƒç»„ä»¶
    ä½¿ç”¨ç§»åŠ¨å¹³å‡åˆ†ç¦»è¶‹åŠ¿å’Œå­£èŠ‚é¡¹
    """
    def __init__(self, kernel_size=25):
        super().__init__()
        self.kernel_size = kernel_size
        
    def forward(self, x):
        """
        è¾“å…¥: x [batch, seq_len, d_model]
        è¾“å‡º: seasonal, trend
        """
        batch_size, seq_len, hidden = x.shape
        kernel_size = min(self.kernel_size, seq_len)
        if kernel_size < 1:
            kernel_size = 1
        # åŠ¨æ€å¹³å‡ï¼ˆä½¿ç”¨å‡½æ•°å¼ä»¥é€‚åº”ä»»æ„åºåˆ—é•¿åº¦ï¼‰
        x_transposed = x.transpose(1, 2)
        padding = max((kernel_size - 1) // 2, 0)
        trend = F.avg_pool1d(
            x_transposed,
            kernel_size=kernel_size,
            stride=1,
            padding=padding,
            count_include_pad=False
        )
        if trend.shape[-1] != seq_len:
            trend = F.interpolate(trend, size=seq_len, mode='linear', align_corners=False)
        trend = trend.transpose(1, 2)
        seasonal = x - trend
        return seasonal, trend


class AutoCorrelation(nn.Module):
    """
    è‡ªç›¸å…³æœºåˆ¶ - Autoformerçš„æ ¸å¿ƒæ³¨æ„åŠ›æœºåˆ¶
    ä½¿ç”¨FFTè®¡ç®—è‡ªç›¸å…³ï¼ŒTop-kæ—¶é—´å»¶è¿Ÿèšåˆ
    """
    def __init__(self, d_model, n_heads, factor=5):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.factor = factor
        self.d_k = d_model // n_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
    def time_delay_agg_training(self, values, corr):
        """
        æ—¶é—´å»¶è¿Ÿèšåˆ - åŸºäºè‡ªç›¸å…³çš„Top-ké€‰æ‹©
        """
        batch, head, length, channel = values.shape
        
        # æ‰¾åˆ°Top-kç›¸å…³æ€§çš„æ—¶é—´å»¶è¿Ÿ
        top_k = int(self.factor * np.log(length)) if length > 1 else 1
        top_k = max(1, min(top_k, length))  # ç¡®ä¿top_kä¸è¶…è¿‡length
        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)  # [batch, length]
        mean_across_batch = torch.mean(mean_value, dim=0)  # [length]
        # ç¡®ä¿ä¸ä¼šç´¢å¼•è¶Šç•Œ
        actual_k = min(top_k, mean_across_batch.size(0))
        index = torch.topk(mean_across_batch, actual_k, dim=-1)[1]
        weights = torch.stack([mean_value[:, idx] for idx in index], dim=-1)  # [batch, top_k]
        
        # å½’ä¸€åŒ–æƒé‡
        weights = torch.softmax(weights, dim=-1)
        
        # åŸºäºå»¶è¿Ÿçš„å€¼èšåˆ
        tmp_corr = torch.softmax(corr, dim=-1)
        tmp_values = values.repeat(1, 1, 2, 1)  # [batch, head, 2*length, channel]
        delays_agg = torch.zeros_like(values).float()  # [batch, head, length, channel]
        
        for i in range(actual_k):
            pattern = torch.roll(tmp_values, -int(index[i]), dims=2)
            delays_agg = delays_agg + pattern[:, :, :length, :] * weights[:, i:i+1].unsqueeze(1).unsqueeze(-1)
        
        return delays_agg
    
    def time_delay_agg_inference(self, values, corr):
        """
        æ¨ç†æ—¶çš„æ—¶é—´å»¶è¿Ÿèšåˆï¼ˆèšåˆç‰ˆæœ¬ï¼Œé¿å…ç´¢å¼•é—®é¢˜ï¼‰
        """
        batch, head, length, channel = values.shape
        
        # æ‰¾åˆ°æœ€å¤§ç›¸å…³çš„å»¶è¿Ÿ
        top_k = int(self.factor * np.log(length)) if length > 1 else 1
        top_k = max(1, min(top_k, length))
        
        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)  # [batch, length]
        mean_across_batch = torch.mean(mean_value, dim=0)
        actual_k = min(top_k, mean_across_batch.size(0))
        indices = torch.topk(mean_across_batch, actual_k, dim=-1)[1]
        selected = mean_value[:, indices]
        weights = torch.softmax(selected, dim=-1)  # [batch, actual_k]
        
        tmp_values = values.repeat(1, 1, 2, 1)
        delays_agg = torch.zeros_like(values).float()
        
        for i in range(actual_k):
            delay_idx = int(indices[i].item())
            pattern = torch.roll(tmp_values, -delay_idx, dims=2)
            delays_agg = delays_agg + pattern[:, :, :length, :] * weights[:, i:i+1].unsqueeze(1).unsqueeze(-1)
        
        return delays_agg
    
    def forward(self, q, k, v):
        """
        è‡ªç›¸å…³æ³¨æ„åŠ›è®¡ç®—
        q, k, v: [batch, length, d_model]
        """
        B, L, D = q.shape
        H = self.n_heads
        d_k = D // H
        
        # çº¿æ€§æŠ•å½±
        Q = self.q_proj(q).view(B, L, H, d_k).transpose(1, 2)  # [B, H, L, d_k]
        K = self.k_proj(k).view(B, L, H, d_k).transpose(1, 2)
        V = self.v_proj(v).view(B, L, H, d_k).transpose(1, 2)
        
        # è½¬ä¸ºfloat32è¿›è¡ŒFFTï¼ˆé¿å…halfç²¾åº¦é™åˆ¶ï¼‰
        Q = Q.float()
        K = K.float()
        V = V.float()
        
        # ä½¿ç”¨FFTè®¡ç®—è‡ªç›¸å…³
        # 1. FFTå˜æ¢
        Q_fft = torch.fft.rfft(Q, dim=2)
        K_fft = torch.fft.rfft(K, dim=2)
        
        # 2. è®¡ç®—è‡ªç›¸å…³ï¼ˆé¢‘åŸŸä¹˜æ³•ï¼‰
        corr = Q_fft * torch.conj(K_fft)
        
        # 3. é€†FFTå›æ—¶åŸŸ
        R = torch.fft.irfft(corr, n=L, dim=2)  # [B, H, L, d_k]
        
        # 4. Top-kæ—¶é—´å»¶è¿Ÿèšåˆ
        if self.training:
            V_agg = self.time_delay_agg_training(V, R)
        else:
            V_agg = self.time_delay_agg_inference(V, R)
        
        # 5. è¾“å‡ºæŠ•å½±
        V_agg = V_agg.transpose(1, 2).contiguous().view(B, L, D)
        output = self.out_proj(V_agg)
        
        return output


class AutoformerAttention(nn.Module):
    """
    Autoformeræ³¨æ„åŠ›å±‚
    ç»“åˆè‡ªç›¸å…³æœºåˆ¶å’Œåºåˆ—åˆ†è§£
    """
    def __init__(self, d_model, n_heads, factor=5):
        super().__init__()
        self.auto_correlation = AutoCorrelation(d_model, n_heads, factor)
        self.decomp1 = SeriesDecomp(kernel_size=25)
        self.decomp2 = SeriesDecomp(kernel_size=25)
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        """
        x: [batch, seq_len, d_model]
        """
        residual = x
        x = self.norm(x)
        
        # åºåˆ—åˆ†è§£1
        seasonal, trend = self.decomp1(x)
        
        # è‡ªç›¸å…³æ³¨æ„åŠ›ï¼ˆä½œç”¨äºå­£èŠ‚é¡¹ï¼‰
        seasonal_out = self.auto_correlation(seasonal, seasonal, seasonal)
        
        # æ®‹å·®è¿æ¥å’Œç¬¬äºŒæ¬¡åˆ†è§£
        x = residual + seasonal_out
        seasonal_out, trend_out = self.decomp2(x)
        
        return seasonal_out + trend_out


class EnhancedFeatureLearning(nn.Module):
    """å¢å¼ºç‰¹å¾å­¦ä¹ æ¨¡å—"""
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # ç‰¹å¾å˜æ¢ç½‘ç»œ
        self.feature_transform = nn.Sequential(
            nn.Linear(input_dim, output_dim * 2),
            nn.LayerNorm(output_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim)
        )
        
        # ç‰¹å¾å¢å¼º
        self.feature_enhance = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
    def forward(self, x):
        batch_size, seq_len, input_features = x.shape
        x_flat = x.reshape(-1, input_features)
        features = self.feature_transform(x_flat)
        enhanced = self.feature_enhance(features)
        output = enhanced.reshape(batch_size, seq_len, self.output_dim)
        return output


class MultiScaleConv(nn.Module):
    """å¤šå°ºåº¦å·ç§¯æ¨¡å—"""
    def __init__(self, d_model):
        super().__init__()
        
        # å¤šå°ºåº¦å·ç§¯åˆ†æ”¯
        self.conv3 = nn.Conv1d(d_model, d_model//3, kernel_size=3, padding='same')
        self.conv5 = nn.Conv1d(d_model, d_model//3, kernel_size=5, padding='same')
        self.conv7 = nn.Conv1d(d_model, d_model//3, kernel_size=7, padding='same')
        
        # æ‰©å¼ å·ç§¯
        self.dilated_conv = nn.Conv1d(d_model, d_model//3, kernel_size=3, dilation=2, padding='same')
        
        # ç‰¹å¾èåˆ
        self.fusion = nn.Conv1d(d_model//3 * 4, d_model, kernel_size=1)
        
        # æ¿€æ´»å’Œæ ‡å‡†åŒ–
        self.activation = nn.GELU()
        self.norm = nn.BatchNorm1d(d_model)
        
    def forward(self, x):
        # x: [batch, seq_len, d_model] -> [batch, d_model, seq_len]
        x_conv = x.transpose(1, 2)
        
        # å¤šå°ºåº¦å·ç§¯
        conv3_out = self.activation(self.conv3(x_conv))
        conv5_out = self.activation(self.conv5(x_conv))
        conv7_out = self.activation(self.conv7(x_conv))
        dilated_out = self.activation(self.dilated_conv(x_conv))
        
        # æ‹¼æ¥å’Œèåˆ
        concat_features = torch.cat([conv3_out, conv5_out, conv7_out, dilated_out], dim=1)
        fused = self.fusion(concat_features)
        fused = self.norm(fused)
        fused = self.activation(fused)
        
        # è½¬å›åºåˆ—æ ¼å¼
        output = fused.transpose(1, 2)
        return output


class SimplifiedMambaBlock(nn.Module):
    """ç®€åŒ–çš„Mambaå— - ä¿æŒMambaçš„æ ¸å¿ƒæ€æƒ³"""
    def __init__(self, d_model, d_state=16):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        
        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(d_model, d_model * 2)
        
        # å·ç§¯å±‚ (ç®€åŒ–çš„çŠ¶æ€ç©ºé—´å¤„ç†)
        self.conv1d = nn.Conv1d(
            d_model, d_model, 
            kernel_size=3, padding='same',
            groups=d_model
        )
        
        # é—¨æ§æœºåˆ¶
        self.gate_proj = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.Sigmoid()
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(d_model, d_model)
        
        # æ ‡å‡†åŒ–
        self.norm = nn.LayerNorm(d_model)
        
    def forward(self, x):
        residual = x
        batch_size, seq_len, _ = x.shape
        
        # è¾“å…¥æŠ•å½±å’Œåˆ†å‰²
        x_proj = self.input_proj(self.norm(x))
        x_conv, x_gate = x_proj.chunk(2, dim=-1)
        
        # å·ç§¯å¤„ç† (æ¨¡æ‹ŸçŠ¶æ€ç©ºé—´)
        x_conv_t = x_conv.transpose(1, 2)  # [B, D, L]
        conv_out = self.conv1d(x_conv_t)
        conv_out = conv_out.transpose(1, 2)  # [B, L, D]
        conv_out = F.silu(conv_out)
        
        # é—¨æ§æœºåˆ¶
        gate = self.gate_proj(x_gate)
        gated_out = conv_out * gate
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(gated_out)
        
        return residual + output


class AutoMamformerBlock(nn.Module):
    """
    Auto-Mamformerå— - Mamba + Autoformeræ··åˆæ¶æ„
    """
    def __init__(self, d_model, n_heads=8, dropout=0.1):
        super().__init__()
        
        # Mambaåˆ†æ”¯ï¼šçŠ¶æ€ç©ºé—´å»ºæ¨¡
        self.mamba = SimplifiedMambaBlock(d_model)
        
        # Autoformeråˆ†æ”¯ï¼šè‡ªç›¸å…³ + åºåˆ—åˆ†è§£
        self.autoformer_attn = AutoformerAttention(d_model, n_heads)
        
        # é—¨æ§èåˆ
        self.gate = nn.Parameter(torch.tensor(0.5))
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout)
        )
        
        # FFNåçš„åºåˆ—åˆ†è§£
        self.decomp_ffn = SeriesDecomp(kernel_size=25)
        
    def forward(self, x):
        # Mambaåˆ†æ”¯
        mamba_out = self.mamba(x)
        
        # Autoformeråˆ†æ”¯
        autoformer_out = self.autoformer_attn(x)
        
        # é—¨æ§èåˆ
        fused = self.gate * mamba_out + (1 - self.gate) * autoformer_out
        
        # å‰é¦ˆç½‘ç»œ + åºåˆ—åˆ†è§£
        ffn_out = self.ffn(fused)
        seasonal, trend = self.decomp_ffn(fused + ffn_out)
        output = seasonal + trend
        
        return output


class AutoMamformerModel(nn.Module):
    """
    Auto-Mamformeræ¨¡å‹ - CODé¢„æµ‹ç‰ˆæœ¬
    Mamba + Autoformeræ··åˆæ¶æ„
    """
    def __init__(self, input_dim, d_model=128, n_layers=4, seq_len=24, pred_len=1, dropout=0.15):
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.d_model = d_model
        
        # 1. å¢å¼ºç‰¹å¾å­¦ä¹ 
        self.feature_learning = EnhancedFeatureLearning(input_dim, d_model)
        
        # 2. ä½ç½®ç¼–ç 
        self.pos_embedding = nn.Parameter(torch.randn(seq_len, d_model) * 0.01)
        
        # 3. Auto-Mamformerå±‚
        self.layers = nn.ModuleList([
            AutoMamformerBlock(d_model, n_heads=8, dropout=dropout)
            for _ in range(n_layers)
        ])
        
        # 4. ç‰¹å¾èšåˆ
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        
        # 5. é¢„æµ‹å¤´
        self.prediction_head = nn.Sequential(
            nn.Linear(d_model * 3, d_model),
            nn.LayerNorm(d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, pred_len)
        )
        
        # 6. æ®‹å·®é¢„æµ‹
        self.linear_residual = nn.Linear(input_dim, pred_len)
        self.ar_residual = nn.Linear(1, pred_len)
        
        # 7. èåˆæƒé‡
        self.fusion_weights = nn.Parameter(torch.tensor([0.8, 0.15, 0.05]))
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight, gain=0.5)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
    
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        x_raw = x
        
        # 1. å¢å¼ºç‰¹å¾å­¦ä¹ 
        features = self.feature_learning(x)
        
        # 2. ä½ç½®ç¼–ç 
        features = features + self.pos_embedding[:seq_len].unsqueeze(0)
        
        # 3. é€šè¿‡Auto-Mamformerå±‚
        for layer in self.layers:
            features = layer(features)
        
        # 4. å¤šå±‚æ¬¡ç‰¹å¾èšåˆ
        seq_features = features[:, -1, :]
        features_conv = features.transpose(1, 2)
        global_avg = self.global_pool(features_conv).squeeze(-1)
        global_max = self.max_pool(features_conv).squeeze(-1)
        
        combined_features = torch.cat([seq_features, global_avg, global_max], dim=1)
        
        # 5. ä¸»é¢„æµ‹
        main_pred = self.prediction_head(combined_features)
        
        # 6. æ®‹å·®é¢„æµ‹
        linear_pred = self.linear_residual(x_raw[:, -1, :])
        ar_pred = self.ar_residual(x_raw[:, -1, -1].unsqueeze(-1))
        
        # 7. æ™ºèƒ½èåˆ
        weights = F.softmax(self.fusion_weights, dim=0)
        final_pred = (weights[0] * main_pred + 
                     weights[1] * linear_pred + 
                     weights[2] * ar_pred)
        
        return final_pred


class TimeSeriesDataset(Dataset):
    """æ—¶é—´åºåˆ—æ•°æ®é›†ç±»"""
    def __init__(self, data, seq_len=24, pred_len=1, augment=False):
        self.data = data
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.augment = augment

        # åˆ›å»ºåºåˆ—
        self.sequences = []
        self.targets = []

        for i in range(len(data) - seq_len + 1):
            seq = data[i:i+seq_len, :-1]  # æ’é™¤æœ€åä¸€åˆ—ï¼ˆç›®æ ‡å˜é‡ï¼‰
            target = data[i+seq_len-1, -1]
            self.sequences.append(seq)
            self.targets.append(target)

        self.sequences = np.array(self.sequences)
        self.targets = np.array(self.targets).reshape(-1, pred_len)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq = self.sequences[idx]
        target = self.targets[idx]

        # æ•°æ®å¢å¼º
        if self.augment and np.random.random() < 0.4:
            noise = np.random.normal(0, 0.01, seq.shape)
            seq = seq + noise

        return torch.FloatTensor(seq), torch.FloatTensor(target)


def analyze_data_and_select_features(file_path):
    """
    åˆ†æåºŸæ°´æ•°æ®å¹¶é€‰æ‹©å…³é”®è¾“å…¥ç‰¹å¾
    """
    print("=" * 60)
    print("åºŸæ°´å¤„ç†æ•°æ®åˆ†æä¸ç‰¹å¾é€‰æ‹©")
    print("=" * 60)
    
    # è¯»å–æ•°æ®
    data = pd.read_csv(file_path)
    print(f"\nåŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"åˆ—å: {list(data.columns)}")
    
    # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    print("\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
    print(data.describe())
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    missing_values = data.isnull().sum()
    print(missing_values[missing_values > 0])
    
    # è®¡ç®—ä¸COD-Sçš„ç›¸å…³æ€§
    print("\nä¸COD-Sçš„ç›¸å…³æ€§åˆ†æ:")
    if 'COD-S' in data.columns:
        correlations = data.corr()['COD-S'].sort_values(ascending=False)
        print(correlations)
        
        # å¯è§†åŒ–ç›¸å…³æ€§
        plt.figure(figsize=(12, 8))
        correlations[1:21].plot(kind='barh')
        plt.title('Top 20 Features Correlated with COD-S')
        plt.xlabel('Correlation Coefficient')
        plt.tight_layout()
        plt.savefig('result/cod_feature_correlation.png', dpi=300, bbox_inches='tight')
        print("\nç›¸å…³æ€§å›¾å·²ä¿å­˜è‡³: result/cod_feature_correlation.png")
        
        # æ¨èä½¿ç”¨å…¨éƒ¨å¯ç”¨ç‰¹å¾ï¼ˆä¿ç•™å®Œæ•´å·¥è‰ºä¿¡æ¯ï¼‰
        available_features = list(data.columns)
        if 'COD-S' in available_features:
            available_features = [col for col in available_features if col != 'COD-S'] + ['COD-S']
        print(f"\nä½¿ç”¨å…¨éƒ¨ç‰¹å¾è¿›è¡Œå»ºæ¨¡ï¼ˆæ€»è®¡ {len(available_features)-1} ä¸ªè¾“å…¥ç‰¹å¾ï¼‰")
        for feat in available_features[:-1]:
            if feat in correlations.index:
                print(f"  - {feat}: ç›¸å…³ç³»æ•° = {correlations[feat]:.3f}")
        
        return data, available_features
    else:
        print("\nè­¦å‘Š: æ•°æ®ä¸­æœªæ‰¾åˆ°COD-Såˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
        return data, None


def preprocess_wastewater_data(data, selected_features):
    """
    åºŸæ°´æ•°æ®é¢„å¤„ç†
    """
    print("\nå¼€å§‹åºŸæ°´æ•°æ®é¢„å¤„ç†...")
    
    # é€‰æ‹©ç‰¹å¾
    feature_data = data[selected_features].copy()
    
    # è½¬æ¢æ•°æ®ç±»å‹
    for col in feature_data.columns:
        feature_data[col] = pd.to_numeric(feature_data[col], errors='coerce')
    
    # å¤„ç†ç¼ºå¤±å€¼
    print(f"\nç¼ºå¤±å€¼å¤„ç†å‰: {feature_data.isnull().sum().sum()} ä¸ªç¼ºå¤±å€¼")
    
    # ä½¿ç”¨å‰å‘å¡«å……å’Œåå‘å¡«å……
    feature_data = feature_data.fillna(method='ffill').fillna(method='bfill')
    
    # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨åˆ—å‡å€¼å¡«å……
    if feature_data.isnull().sum().sum() > 0:
        feature_data = feature_data.fillna(feature_data.mean())
    
    print(f"ç¼ºå¤±å€¼å¤„ç†å: {feature_data.isnull().sum().sum()} ä¸ªç¼ºå¤±å€¼")
    
    # å»é™¤å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨3ÏƒåŸåˆ™ï¼‰
    print("\nå¼‚å¸¸å€¼å¤„ç†...")
    original_len = len(feature_data)
    for col in feature_data.columns:
        mean = feature_data[col].mean()
        std = feature_data[col].std()
        # å°†å¼‚å¸¸å€¼è®¾ä¸ºè¾¹ç•Œå€¼è€Œä¸æ˜¯åˆ é™¤
        feature_data[col] = feature_data[col].clip(mean - 3*std, mean + 3*std)
    
    print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {feature_data.shape}")
    print(f"\nCOD-Sç»Ÿè®¡ä¿¡æ¯:\n{feature_data['COD-S'].describe()}")
    
    return feature_data


def apply_feature_engineering(data, start_idx=0, target_col='COD-S'):
    """
    åº”ç”¨ç‰¹å¾å·¥ç¨‹ - é’ˆå¯¹åºŸæ°´å¤„ç†æ•°æ®
    """
    print(f"\nå¯¹ç´¢å¼• {start_idx} å¼€å§‹çš„æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹...")
    
    feature_data = data.copy()
    
    # è·å–æ‰€æœ‰éç›®æ ‡åˆ—
    input_cols = [col for col in feature_data.columns if col != target_col]
    
    # 1. æ»åç‰¹å¾ - æ•æ‰å†å²ä¿¡æ¯
    lag_windows = [1, 2, 3, 6, 12]
    lag_columns = [
        target_col,
        'SS-S', 'BOD-S', 'BOD-D', 'COD-D',
        'BOD-E', 'COD-E', 'SS-E', 'BOD-P', 'SS-P'
    ]
    for col in lag_columns:
        if col in feature_data.columns:
            for lag in lag_windows:
                feature_data[f'{col}_lag_{lag}'] = feature_data[col].shift(lag)
    
    # 2. ç§»åŠ¨å¹³å‡ç‰¹å¾ - æ•æ‰è¶‹åŠ¿
    ma_windows = [3, 6, 12, 24]
    ma_columns = ['COD-S', 'SS-S', 'BOD-D', 'COD-D', 'BOD-E', 'COD-E']
    for col in ma_columns:
        if col in feature_data.columns:
            for window in ma_windows:
                feature_data[f'{col}_ma_{window}'] = feature_data[col].rolling(
                    window=window, min_periods=1).mean()
    
    # 3. å·®åˆ†ç‰¹å¾ - æ•æ‰å˜åŒ–ç‡
    diff_columns = ['COD-S', 'SS-S', 'BOD-D', 'COD-D', 'BOD-E', 'COD-E']
    for col in diff_columns:
        if col in feature_data.columns:
            feature_data[f'{col}_diff_1'] = feature_data[col].diff(1)
            feature_data[f'{col}_diff_3'] = feature_data[col].diff(3)
    
    # 4. æ¯”ç‡ç‰¹å¾ - åºŸæ°´å¤„ç†å…³é”®æŒ‡æ ‡
    if 'BOD-E' in feature_data.columns and 'COD-E' in feature_data.columns:
        feature_data['BOD_COD_ratio_E'] = feature_data['BOD-E'] / (feature_data['COD-E'] + 1e-8)
    
    if 'BOD-P' in feature_data.columns and 'COD-P' in feature_data.columns:
        feature_data['BOD_COD_ratio_P'] = feature_data['BOD-P'] / (feature_data['COD-P'] + 1e-8)
    
    if 'BOD-D' in feature_data.columns and 'COD-D' in feature_data.columns:
        feature_data['BOD_COD_ratio_D'] = feature_data['BOD-D'] / (feature_data['COD-D'] + 1e-8)
    
    if 'SS-E' in feature_data.columns and 'VSS-E' in feature_data.columns:
        feature_data['VSS_SS_ratio_E'] = feature_data['VSS-E'] / (feature_data['SS-E'] + 1e-8)
    
    if 'SS-D ' in feature_data.columns and ' VSS-D' in feature_data.columns:
        feature_data['VSS_SS_ratio_D'] = feature_data[' VSS-D'] / (feature_data['SS-D '] + 1e-8)
    
    # 5. å»é™¤æ•ˆç‡ç‰¹å¾
    if 'BOD-E' in feature_data.columns and 'BOD-P' in feature_data.columns:
        feature_data['BOD_removal_E_to_P'] = (feature_data['BOD-E'] - feature_data['BOD-P']) / (feature_data['BOD-E'] + 1e-8)
    
    if 'BOD-P' in feature_data.columns and 'BOD-D' in feature_data.columns:
        feature_data['BOD_removal_P_to_D'] = (feature_data['BOD-P'] - feature_data['BOD-D']) / (feature_data['BOD-P'] + 1e-8)
    
    if 'BOD-D' in feature_data.columns and 'BOD-S' in feature_data.columns:
        feature_data['BOD_removal_D_to_S'] = (feature_data['BOD-D'] - feature_data['BOD-S']) / (feature_data['BOD-D'] + 1e-8)
    
    if 'COD-E' in feature_data.columns and 'COD-D' in feature_data.columns:
        feature_data['COD_removal_E_to_D'] = (feature_data['COD-E'] - feature_data['COD-D']) / (feature_data['COD-E'] + 1e-8)
    
    if 'COD-D' in feature_data.columns and 'COD-S' in feature_data.columns:
        feature_data['COD_removal_D_to_S'] = (feature_data['COD-D'] - feature_data['COD-S']) / (feature_data['COD-D'] + 1e-8)
    
    if 'SS-E' in feature_data.columns and 'SS-D ' in feature_data.columns:
        feature_data['SS_removal_E_to_D'] = (feature_data['SS-E'] - feature_data['SS-D ']) / (feature_data['SS-E'] + 1e-8)
    
    if 'SS-D ' in feature_data.columns and 'SS-S' in feature_data.columns:
        feature_data['SS_removal_D_to_S'] = (feature_data['SS-D '] - feature_data['SS-S']) / (feature_data['SS-D '] + 1e-8)

    # ç§»é™¤NaNè¡Œ
    before_dropna = len(feature_data)
    feature_data = feature_data.dropna()
    dropped = before_dropna - len(feature_data)
    
    print(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®å½¢çŠ¶: {feature_data.shape}")
    print(f"å› NaNåˆ é™¤çš„è¡Œæ•°: {dropped}")
    
    return feature_data


def create_data_splits(
    data,
    seq_len=24,
    test_size=0.2,
    augment_factor=1,
    target_col='COD-S',
    use_feature_engineering=True,
    top_feature_count=None
):
    """
    åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ† - å°†æ•´ä¸ªæ•°æ®é›†ç”Ÿæˆåºåˆ—åéšæœºåˆ’åˆ†
    """
    print(f"\nåˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®ï¼ˆæ•´ä½“ç‰¹å¾å·¥ç¨‹ + éšæœºåˆ’åˆ†ï¼‰...")
    
    if use_feature_engineering:
        engineered_data = apply_feature_engineering(data, start_idx=0, target_col=target_col)
        engineered_data = engineered_data.dropna().reset_index(drop=True)
        if top_feature_count is not None and top_feature_count > 0:
            print(f"é€‰æ‹©ä¸{target_col}æœ€ç›¸å…³çš„å‰ {top_feature_count} ä¸ªç‰¹å¾...")
            correlation = engineered_data.corr().abs()[target_col].sort_values(ascending=False)
            selected_features = [col for col in correlation.index if col != target_col][:top_feature_count]
            engineered_data = engineered_data[selected_features + [target_col]]
    else:
        engineered_data = data.copy().reset_index(drop=True)
    
    print(f"ç‰¹å¾å·¥ç¨‹åæ•°æ®å½¢çŠ¶: {engineered_data.shape}")
    print(f"è¾“å…¥ç‰¹å¾æ•°é‡: {engineered_data.shape[1] - 1}")
    
    # æ ‡å‡†åŒ–
    scaler = RobustScaler(quantile_range=(5.0, 95.0))
    scaled_values = scaler.fit_transform(engineered_data.values)
    
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    base_dataset_eval = TimeSeriesDataset(scaled_values, seq_len=seq_len, augment=False)
    base_dataset_train = TimeSeriesDataset(scaled_values, seq_len=seq_len, augment=(augment_factor > 1))
    total_samples = len(base_dataset_eval)
    print(f"å¯ç”¨åºåˆ—æ€»æ•°: {total_samples}")
    
    test_samples = max(1, int(total_samples * test_size))
    train_samples = total_samples - test_samples
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {train_samples} | æµ‹è¯•æ ·æœ¬æ•°: {test_samples}")
    
    g = torch.Generator()
    g.manual_seed(42)
    all_indices = torch.randperm(total_samples, generator=g)
    test_indices = all_indices[:test_samples].tolist()
    train_indices = all_indices[test_samples:].tolist()
    
    train_dataset = torch.utils.data.Subset(base_dataset_train, train_indices)
    test_dataset = torch.utils.data.Subset(base_dataset_eval, test_indices)
    
    return train_dataset, test_dataset, scaler, engineered_data.shape[1] - 1, engineered_data, train_indices, test_indices


def train_final_model(model, train_loader, val_loader, epochs=60, lr=0.001, patience=12):
    """è®­ç»ƒæ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    use_cuda = device.type == 'cuda'
    gpu_name = None
    
    if use_cuda:
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"æ£€æµ‹åˆ°GPU: {gpu_name}")
        print(f"æ˜¾å­˜æ€»é‡: {torch.cuda.get_device_properties(current_device).total_memory / 1024**3:.2f} GB")
    else:
        print("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œè®­ç»ƒå°†åœ¨CPUä¸Šè¿›è¡Œã€‚")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        weight_decay=5e-4,
        betas=(0.9, 0.95)
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=15, T_mult=2, eta_min=lr*0.01
    )
    
    # æŸå¤±å‡½æ•°
    mse_criterion = nn.MSELoss()
    huber_criterion = nn.SmoothL1Loss(beta=0.5)
    
    scaler = GradScaler(enabled=use_cuda)
    
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    
    print(f"\nå¼€å§‹è®­ç»ƒAuto-Mamformeræ¨¡å‹ï¼Œè®¾å¤‡: {device}")
    
    for epoch in range(epochs):
        if use_cuda:
            torch.cuda.synchronize()
        epoch_start_time = time.time()
        
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        batch_count = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device, non_blocking=True)
            batch_y = batch_y.to(device, non_blocking=True)
            
            optimizer.zero_grad()
            
            amp_context = autocast(dtype=torch.float16) if use_cuda else nullcontext()
            with amp_context:
                predictions = model(batch_x)
                
                # æ··åˆæŸå¤±
                mse_loss = mse_criterion(predictions.squeeze(), batch_y.squeeze())
                huber_loss = huber_criterion(predictions.squeeze(), batch_y.squeeze())
                loss = 0.7 * mse_loss + 0.3 * huber_loss
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            batch_count += 1
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_preds = []
        val_trues = []
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x = batch_x.to(device, non_blocking=True)
                batch_y = batch_y.to(device, non_blocking=True)
                
                amp_context = autocast(dtype=torch.float16) if use_cuda else nullcontext()
                with amp_context:
                    predictions = model(batch_x)
                    loss = mse_criterion(predictions.squeeze(), batch_y.squeeze())
                
                val_loss += loss.item()
                val_preds.append(predictions.detach().cpu())
                val_trues.append(batch_y.detach().cpu())
        
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        val_preds = torch.cat(val_preds).squeeze().numpy()
        val_trues = torch.cat(val_trues).squeeze().numpy()
        val_r2 = r2_score(val_trues, val_preds)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        scheduler.step()
        
        # è®¡ç®—æœ¬è½®è®­ç»ƒæ—¶é—´
        if use_cuda:
            torch.cuda.synchronize()
        epoch_time = time.time() - epoch_start_time
        
        # æ—©åœå’Œä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'model/auto_mamformer_cod.pth')
        else:
            patience_counter += 1
        
        print(f'Epoch [{epoch + 1:2d}/{epochs}] '
              f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | R2: {val_r2:.4f} | '
              f'LR: {optimizer.param_groups[0]["lr"]:.6f} | Time: {epoch_time:.2f}s'
              f'{" | GPU: " + gpu_name if gpu_name else ""}')
        
        if patience_counter >= patience:
            print(f"æ—©åœ: {patience}è½®æ— æ”¹å–„")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('model/auto_mamformer_cod.pth'))
    return model, train_losses, val_losses


def get_rescaled_predictions(model, data_loader, scaler, return_features=False):
    """è·å–æ¨¡å‹é¢„æµ‹çš„åæ ‡å‡†åŒ–ç»“æœ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    
    predictions = []
    true_values = []
    features_list = []
    
    with torch.no_grad():
        for batch_x, batch_y in data_loader:
            batch_x = batch_x.to(device, non_blocking=True)
            batch_y = batch_y.to(device, non_blocking=True)
            output = model(batch_x)
            predictions.extend(output.cpu().numpy().flatten())
            true_values.extend(batch_y.cpu().numpy().flatten())
            if return_features:
                last_step = batch_x[:, -1, :].cpu().numpy()
                features_list.append(last_step)
    
    predictions = np.array(predictions)
    true_values = np.array(true_values)
    
    dummy_pred = np.zeros((len(predictions), scaler.n_features_in_))
    dummy_true = np.zeros((len(true_values), scaler.n_features_in_))
    dummy_pred[:, -1] = predictions
    dummy_true[:, -1] = true_values
    
    predictions_rescaled = scaler.inverse_transform(dummy_pred)[:, -1]
    true_values_rescaled = scaler.inverse_transform(dummy_true)[:, -1]
    
    if return_features:
        features_scaled = np.vstack(features_list)
        dummy_feat = np.zeros((features_scaled.shape[0], scaler.n_features_in_))
        dummy_feat[:, :-1] = features_scaled
        features_rescaled = scaler.inverse_transform(dummy_feat)[:, :-1]
        return predictions_rescaled, true_values_rescaled, features_rescaled
    
    return predictions_rescaled, true_values_rescaled


def evaluate_model(model, test_loader, scaler, calibrator=None, external_features=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    predictions_rescaled, true_values_rescaled = get_rescaled_predictions(
        model, test_loader, scaler, return_features=False
    )
    
    if calibrator is not None:
        if external_features is None:
            raise ValueError("external_features must be provided when calibrator is used")
        X_calib = np.hstack([predictions_rescaled.reshape(-1, 1), external_features])
        predictions_calibrated = calibrator.predict(X_calib).flatten()
    else:
        predictions_calibrated = predictions_rescaled
    
    r2 = r2_score(true_values_rescaled, predictions_calibrated)
    mse = mean_squared_error(true_values_rescaled, predictions_calibrated)
    mae = mean_absolute_error(true_values_rescaled, predictions_calibrated)
    rmse = np.sqrt(mse)
    
    print(f"\næ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"R2 Score: {r2:.4f}")
    print(f"MSE: {mse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")
    
    plt.figure(figsize=(14, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(true_values_rescaled[:200], label='True COD-S', alpha=0.7)
    plt.plot(predictions_calibrated[:200], label='Predicted COD-S', alpha=0.7)
    plt.xlabel('Sample Index')
    plt.ylabel('COD-S (mg/L)')
    plt.title('Auto-Mamformer: COD-S Prediction Results (First 200 Samples)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.scatter(true_values_rescaled, predictions_calibrated, alpha=0.5)
    plt.plot([true_values_rescaled.min(), true_values_rescaled.max()],
             [true_values_rescaled.min(), true_values_rescaled.max()],
             'r--', lw=2)
    plt.xlabel('True COD-S (mg/L)')
    plt.ylabel('Predicted COD-S (mg/L)')
    plt.title(f'Prediction vs True (R2 = {r2:.4f})')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('result/auto_mamformer_cod_results.png', dpi=300, bbox_inches='tight')
    print("\né¢„æµ‹ç»“æœå›¾å·²ä¿å­˜è‡³: result/auto_mamformer_cod_results.png")
    
    return r2, mse, mae, rmse, predictions_calibrated, true_values_rescaled


def main():
    """ä¸»å‡½æ•°"""
    set_seed(42)
    
    print("=" * 60)
    print("Auto-Mamformer - åºŸæ°´å¤„ç†CODé¢„æµ‹")
    print("Mamba + Autoformeræ··åˆæ¶æ„")
    print("=" * 60)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('model', exist_ok=True)
    os.makedirs('result', exist_ok=True)
    
    # 1. æ•°æ®åˆ†æä¸ç‰¹å¾é€‰æ‹©
    data_path = resolve_water_data_path('water-treatment_model_cleaned.csv')
    print(f"ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_path}")
    data, selected_features = analyze_data_and_select_features(data_path)
    
    if selected_features is None:
        print("é”™è¯¯: æ— æ³•ç»§ç»­ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼")
        return
    
    # 2. æ•°æ®é¢„å¤„ç†
    processed_data = preprocess_wastewater_data(data, selected_features)
    
    # æ¨¡å‹å‚æ•°ï¼ˆè¿›é˜¶ä¼˜åŒ–ä»¥å†²å‡»0.95ï¼‰
    seq_len = 3  # å¢åŠ åºåˆ—é•¿åº¦ä»¥åˆ©ç”¨æ›´å¤šå†å²ä¿¡æ¯
    batch_size = 32
    epochs = 180  # è¿›ä¸€æ­¥å¢åŠ è®­ç»ƒè½®æ•°
    lr = 2e-4  # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ä»¥è·å¾—æ›´ç²¾ç»†æ”¶æ•›
    
    # 3. åˆ›å»ºæ•°æ®é›†ï¼ˆå¯ç”¨ç‰¹å¾å·¥ç¨‹ + å…³é”®ç‰¹å¾é€‰æ‹©ï¼‰
    train_dataset, test_dataset, scaler, input_dim, engineered_data, full_train_indices, test_indices = create_data_splits(
        processed_data,
        seq_len=seq_len,
        test_size=0.2,
        augment_factor=2,
        target_col='COD-S',
        use_feature_engineering=True,
        top_feature_count=60
    )

    print(f"\nå®é™…è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim}")
    
    # 4. åˆ’åˆ†éªŒè¯é›†
    train_size = max(1, int(len(train_dataset) * 0.85))
    loader_train_indices = list(range(train_size))
    loader_val_indices = list(range(train_size, len(train_dataset)))
    
    train_subset = torch.utils.data.Subset(train_dataset, loader_train_indices)
    val_subset = torch.utils.data.Subset(train_dataset, loader_val_indices)
    
    # 5. DataLoader
    import platform
    is_windows = platform.system() == 'Windows'
    
    if is_windows:
        num_workers = 0
        pin_memory = torch.cuda.is_available()
        persistent_workers = False
        prefetch_factor = None
    elif torch.cuda.is_available():
        num_workers = min(8, os.cpu_count() or 4)
        pin_memory = True
        persistent_workers = True
        prefetch_factor = 4
    else:
        num_workers = max(os.cpu_count() - 1, 1)
        pin_memory = False
        persistent_workers = False
        prefetch_factor = 2
    
    g = torch.Generator()
    g.manual_seed(42)
    
    train_loader_kwargs = {
        'batch_size': batch_size,
        'shuffle': True,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'drop_last': True,
        'generator': g
    }
    if not is_windows:
        train_loader_kwargs['persistent_workers'] = persistent_workers
        train_loader_kwargs['prefetch_factor'] = prefetch_factor
    
    val_test_loader_kwargs = {
        'batch_size': batch_size,
        'shuffle': False,
        'num_workers': num_workers,
        'pin_memory': pin_memory
    }
    if not is_windows:
        val_test_loader_kwargs['persistent_workers'] = persistent_workers
        val_test_loader_kwargs['prefetch_factor'] = prefetch_factor
    
    train_loader = DataLoader(train_subset, **train_loader_kwargs)
    val_loader = DataLoader(val_subset, **val_test_loader_kwargs)
    test_loader = DataLoader(test_dataset, **val_test_loader_kwargs)
    
    # 6. åˆ›å»ºAuto-Mamformeræ¨¡å‹ï¼ˆä¼˜åŒ–é…ç½®ï¼‰
    model = AutoMamformerModel(
        input_dim=input_dim,
        d_model=128,  # å¢åŠ æ¨¡å‹å®¹é‡
        n_layers=4,  # å¢åŠ å±‚æ•°
        seq_len=seq_len,
        pred_len=1,
        dropout=0.15  # å¢åŠ dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
    )
    
    print(f"\næ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # 7. è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒ...")
    model, train_losses, val_losses = train_final_model(
        model, train_loader, val_loader, epochs=epochs, lr=lr, patience=30  # å¢åŠ è€å¿ƒå€¼
    )
    
    # 8. è¯„ä¼°æ¨¡å‹ï¼ˆæ— æ ¡å‡†å™¨ï¼‰
    print("\nå¼€å§‹è¯„ä¼°ï¼ˆä»…æ¨¡å‹è¾“å‡ºï¼‰...")
    test_pred_rescaled, test_true_rescaled = get_rescaled_predictions(
        model, test_loader, scaler, return_features=False
    )
    r2 = r2_score(test_true_rescaled, test_pred_rescaled)
    mse = mean_squared_error(test_true_rescaled, test_pred_rescaled)
    mae = mean_absolute_error(test_true_rescaled, test_pred_rescaled)
    rmse = np.sqrt(mse)
    mask = np.abs(test_true_rescaled) > 1e-8
    mape = float(np.mean(np.abs((test_true_rescaled[mask] - test_pred_rescaled[mask]) / test_true_rescaled[mask])) * 100)

    print(f"\næ¨¡å‹è¯„ä¼°ç»“æœ:")
    print(f"R2 Score: {r2:.4f}")
    print(f"MAPE: {mape:.2f}%")
    print(f"MAE: {mae:.4f}")
    print(f"RMSE: {rmse:.4f}")

    # å¯è§†åŒ–
    plt.figure(figsize=(14, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(test_true_rescaled[:200], label='True COD-S', alpha=0.7)
    plt.plot(test_pred_rescaled[:200], label='Predicted COD-S', alpha=0.7)
    plt.xlabel('Sample Index')
    plt.ylabel('COD-S (mg/L)')
    plt.title('Auto-Mamformer: COD-S Prediction Results (First 200 Samples)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.scatter(test_true_rescaled, test_pred_rescaled, alpha=0.5)
    plt.plot([test_true_rescaled.min(), test_true_rescaled.max()],
             [test_true_rescaled.min(), test_true_rescaled.max()],
             'r--', lw=2)
    plt.xlabel('True COD-S (mg/L)')
    plt.ylabel('Predicted COD-S (mg/L)')
    plt.title(f'Prediction vs True (R2 = {r2:.4f})')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('result/auto_mamformer_cod_results.png', dpi=300, bbox_inches='tight')
    print("\né¢„æµ‹ç»“æœå›¾å·²ä¿å­˜è‡³: result/auto_mamformer_cod_results.png")
    
    predictions = test_pred_rescaled
    true_values = test_true_rescaled

    # 10. ä¿å­˜ç»“æœ
    results = {
        'r2': r2,
        'mape': mape,
        'mae': mae,
        'rmse': rmse,
        'predictions': predictions,
        'true_values': true_values,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'calibrator_type': None
    }
    
    np.save('result/auto_mamformer_cod_results.npy', results)
    print("\nç»“æœå·²ä¿å­˜è‡³: result/auto_mamformer_cod_results.npy")

    # ä¿å­˜/æ›´æ–° water summary JSON
    summary_path = 'result/auto_mamformer_water_summary.json'
    summary = {}
    if os.path.exists(summary_path):
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary = json.load(f)
    summary['cod'] = {
        'r2': float(r2),
        'mape': float(mape),
        'mae': float(mae),
        'rmse': float(rmse)
    }
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"\nWater summaryå·²æ›´æ–°è‡³: {summary_path}")
    
    # 10. è¾“å‡ºæœ€ç»ˆè¯„ä¼°
    print("\n" + "=" * 60)
    print("ğŸ¯ Auto-Mamformeræ¨¡å‹æœ€ç»ˆç»“æœ:")
    print("=" * 60)
    print(f"æµ‹è¯•é›† R2 Score: {r2:.4f}")
    print(f"MAPE: {mape:.2f}%")
    print(f"MAE: {mae:.4f} mg/L")
    print(f"RMSE: {rmse:.4f} mg/L")
    
    if r2 >= 0.85:
        print(f"\nâœ… ä¼˜ç§€ï¼Auto-Mamformeræ¨¡å‹è¾¾åˆ°é«˜æ€§èƒ½æ ‡å‡†ï¼")
    elif r2 >= 0.75:
        print(f"\nâœ… è‰¯å¥½ï¼Auto-Mamformeræ¨¡å‹æ€§èƒ½å¯æ¥å—ï¼")
    else:
        print(f"\nğŸ“ˆ æ¨¡å‹æœ‰æ”¹è¿›ç©ºé—´ï¼Œå»ºè®®è°ƒæ•´ç‰¹å¾æˆ–è¶…å‚æ•°")
    
    print("=" * 60)
    
    return model, results


if __name__ == "__main__":
    import multiprocessing
    multiprocessing.freeze_support()
    
    main()
